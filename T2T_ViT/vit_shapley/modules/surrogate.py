import logging
from typing import Optional

import pytorch_lightning as pl
import torch
import torch.nn as nn
from torchvision import models as cnn_models
import math

import models.t2t_vit
from vit_shapley.modules import surrogate_utils
from utils import load_for_transfer_learning
from vit_shapley.CIFAR_10_Dataset import CIFAR_10_Dataset, CIFAR_10_Datamodule
from collections import OrderedDict
import argparse

torch.set_float32_matmul_precision('medium')

parser = argparse.ArgumentParser(description="Surrogate training")
parser.add_argument("--num_players", required=False, default=16, type=int, help="number of players")

args = parser.parse_args()

class Surrogate(pl.LightningModule):
    """
    `pytorch_lightning` module for surrogate

    Args:
        output_dim: the dimension of output
        target_model: This model will be trained to generate output similar to
            the output generated by 'target_model' for the same input.
        learning_rate: learning rate of optimizer
        weight_decay: weight decay of optimizer
        decay_power: only `cosine` annealing scheduler is supported currently
        warmup_steps: parameter for the `cosine` annealing scheduler
    """

    def __init__(self,
                 output_dim: int,
                 target_model: Optional[nn.Module],
                 learning_rate: Optional[float],
                 weight_decay: Optional[float],
                 decay_power: Optional[str],
                 warmup_steps: Optional[int]):

        super().__init__()
        self.save_hyperparameters(ignore=['target_model'])
        self.target_model = target_model

        # Backbone initialization

        self.backbone = models.t2t_vit.t2t_vit_14(num_classes=10)
        # state_dict = load_state_dict(self.backbone, checkpoint_path='../models/81.5_T2T_ViT_14.pth.tar',
        #                             num_classes=10)
        # self.backbone.load_state_dict(state_dict, strict=False)
        # load_for_transfer_learning(self.backbone, checkpoint_path='../models/81.5_T2T_ViT_14.pth.tar',
        #                             num_classes=10)

        # Nullify classification head built in the backbone module and rebuild.
        head_in_features = self.backbone.head.in_features
        self.backbone.head = nn.Identity()

        self.head = nn.Linear(head_in_features, self.hparams["output_dim"])

        # Set `num_players` variable.
        # self.num_players = 196  # 14 * 14
        self.num_players = args.num_players

        # Set up modules for calculating metric
        surrogate_utils.set_metrics(self)

    def configure_optimizers(self):
        return surrogate_utils.set_schedule(self)

    def forward(self, images, masks):
        assert masks.shape[-1] == self.num_players


        # if images.shape[2:4] == (224, 224) and masks.shape[1] == 196:
        patch_size = int(math.sqrt(self.num_players))
        masks = masks.reshape(-1, patch_size, patch_size)
        num_patches = 224//patch_size
        masks = torch.repeat_interleave(torch.repeat_interleave(masks, num_patches, dim=2), num_patches, dim=1)
        
        
        images_masked = images * masks.unsqueeze(1)    
        out = self.backbone(images_masked)
        logits = self.head(out)
        # [:,0].unsqueeze(1) is not needed
        
        return logits

    def training_step(self, batch, batch_idx):
        assert self.target_model is not None
        images, masks = batch["images"], batch["masks"]

        logits = self(images, masks) # ['logits']
        self.target_model.eval()
        with torch.no_grad():
            logits_target = self.target_model(images.to(self.target_model.device)).to(
                self.device) #['logits']
        loss = surrogate_utils.compute_metrics(self, logits=logits, logits_target=logits_target, phase='train')
        return loss

    def on_training_epoch_end(self):
        surrogate_utils.epoch_wrapup(self, phase='train')

    def validation_step(self, batch, batch_idx):
        assert self.target_model is not None
        images, masks = batch["images"], batch["masks"]
        logits = self(images, masks) #['logits']
        self.target_model.eval()
        with torch.no_grad():
            logits_target = self.target_model(images.to(self.target_model.device)).to(
                self.device) #['logits']
        loss = surrogate_utils.compute_metrics(self, logits=logits, logits_target=logits_target, phase='val')

    def on_validation_epoch_end(self,):
        surrogate_utils.epoch_wrapup(self, phase='val')

    def test_step(self, batch, batch_idx):
        assert self.target_model is not None
        images, masks = batch["images"], batch["masks"]
        logits = self(images, masks) # ['logits']
        self.target_model.eval()
        with torch.no_grad():
            logits_target = self.target_model(images.to(self.target_model.device)).to(
                self.device) #['logits']
        loss = surrogate_utils.compute_metrics(self, logits=logits, logits_target=logits_target, phase='test')

    def on_test_epoch_end(self):
        surrogate_utils.epoch_wrapup(self, phase='test')



if __name__ == "__main__":
    target_model = models.t2t_vit.t2t_vit_14(num_classes=10)

    # num_classes=10
    checkpoint = torch.load('../../checkpoint_cifar10_T2t_vit_14/ckpt_0.01_0.0005_97.5.pth')

    new_state_dict = OrderedDict()

    for k, v in checkpoint.items():
        # strip `module.` prefix
        name = k[7:] if k.startswith('module') else k
        new_state_dict[name] = v

    state_dict = new_state_dict

    target_model.load_state_dict(state_dict, strict=False)

    surrogate = Surrogate(output_dim=10,
                          target_model=target_model,
                          learning_rate=1e-5,
                          weight_decay=0.0,
                          decay_power='cosine',
                          warmup_steps=2)
    

    CIFAR_10 = CIFAR_10_Datamodule(num_players=args.num_players, num_mask_samples=1, paired_mask_samples=False)

    trainer = pl.Trainer(max_epochs=50, 
                         default_root_dir=f"./checkpoints_surrogate_num_players_lala_{surrogate.num_players}")# logger=False)
    trainer.fit(surrogate, CIFAR_10)
